
from elasticsearch import Elasticsearch, helpers

def get_shards_info(es, index):
    # Get information about the shards of the specified index
    try:
        response = es.cat.shards(index=index, format="json", h="shard,node")
        return response
    except Exception as e:
        print(f"Error retrieving shard information: {e}")
        return None

def move_shards_between_indices(es, source_index, target_index):
    # Get information about the shards in the source index
    shards_info = get_shards_info(es, source_index)

    if shards_info is None:
        return None

    # Loop through each shard and perform the reindex operation
    for shard_info in shards_info:
        shard, node = shard_info["shard"], shard_info["node"]

        # Use the _reindex API to move the specific shard from source to target index
        body = {
            "source": {"index": source_index, "query": {"term": {"_id": shard}}},
            "dest": {"index": target_index}
        }

        try:
            response = es.reindex(body=body, request_timeout=600, wait_for_completion=True, wait_for_active_shards="all")
            print(f"Shard {shard} moved successfully from node {node}.")
        except Exception as e:
            print(f"Error moving shard {shard}: {e}")

# Example usage:
if __name__ == "__main__":
    es_host = 'http://localhost:9200'  # Replace with your Elasticsearch server address
    source_index = 'source_index'
    target_index = 'target_index'

    es = Elasticsearch([es_host])

    move_shards_between_indices(es, source_index, target_index)




https://forum.bigfix.com/t/rest-api-script-to-pull-report-from-bigfix-web-report/29996/4


pip config set global.trusted-host \
    "pypi.org files.pythonhosted.org pypi.python.org" \
    --trusted-host=pypi.python.org \
    --trusted-host=pypi.org \
    --trusted-host=files.pythonhosted.org




#!/usr/bin/env python
# coding: utf-8

# In[20]:


import urllib.request

def import_web(ticker):
    print("import_web")
    """
    :param ticker: Takes the company ticker
    :return: Returns the HTML of the page
    """
    url = 'https://www1.nseindia.com/live_market/dynaContent/live_watch/get_quote/GetQuote.jsp?symbol='+ticker+'&illiquid=0&smeFlag=0&itpFlag=0'
    print(url)
    req = urllib.request.Request(url, headers={'User-Agent' : "Chrome Browser"}) 
    fp = urllib.request.urlopen(req, timeout=10)
    mybytes = fp.read()
    mystr = mybytes.decode("utf8")
    fp.close()
    return mystr


# In[9]:


def filter_data(string_html):          
    print("filter_data")
    searchString = 'div id="responseDiv" style="display:none"'
    #assign: stores html tag to find where data starts
    searchString2 = '/div'
    #stores:  stores html tag where  data end
    sta = string_html.find(searchString)
    # returns & store: find() method returns the lowest index of the substring (if found). If not found, it returns -1.
    data = string_html[sta + 43:]
    #returns & stores: skips 43 characters and stores the index of substring
    end = data.find(searchString2)
    # returns & store: find() method returns the lowest index of the substring (if found). If not found, it returns -1.
    fdata = data[:end]
    #fetch: stores the fetched data into fdata
    stripped = fdata.strip()
    #removes: blank spaces
    return stripped


# In[10]:


def get_quote(ticker):
    print("get_quote")
    """
    :param ticker: Takes the company ticker
    :return: None
    """
    ticker = ticker.upper()
    try:
        """fetches a UTF-8-encoded web page, and  extract some text from the HTML"""
        string_html = import_web(ticker)
        get_data(filter_data(string_html),ticker)
    except Exception as e:
        print(e)


# In[11]:


import json

data_infy = {}
data_tcs = {}

def get_data(stripped, company):
    print("get_data")
    js = json.loads(stripped)
    datajs = js['data'][0]
    subdictionary = {}
    subdictionary['1. open'] = datajs['open']
    subdictionary['2. high'] = datajs['dayHigh']
    subdictionary['3. low'] = datajs['dayLow']
    subdictionary['4. close'] = datajs['lastPrice']
    subdictionary['5. volume'] = datajs['totalTradedVolume']
    if company == 'INFY':
        print (
            'Adding value at : ',
            js['lastUpdateTime'],
            ' to ',
            company,
            ' Price:',
            datajs["lastPrice"],
            )
        data_infy[js['lastUpdateTime']] = subdictionary
    elif company == 'TCS':
        print (
            'Adding value at : ',
            js['lastUpdateTime'],
            ' to ',
            company,
            ' Price:',
            datajs["lastPrice"],
            )
        data_tcs[js['lastUpdateTime']] = subdictionary


# In[15]:


import time

lsave=time.time()

def autoSave():
	print("autoSave")
	global lsave
	curr_time = time.time()
	if(curr_time >= lsave + 300):
		with open('infy','a+') as f:
			f.write(str(data_infy))
		with open('tcs','a+') as f:
			f.write(str(data_tcs))
		lsave = time.time()
		combiner()
		print("AutoSaved at : "+ time.strftime('%Y-%m-%d %H:%M:%S',time.localtime(lsave)))


# In[16]:


def combiner():
	print("combiner")
	file_names = ['infy','tcs']

	for ticker in file_names:
		final = {}

		with open(ticker,'r') as f:
			data = f.read()
		data = data.replace("}{","}split{")
		splittedData = data.split('split')
		
		for dictionary in splittedData:
			tmp = json.loads(dictionary.replace("'",'"'))
			for key in tmp.keys():
				final[key] = tmp[key]
	
		newFileName = ticker
		with open(newFileName,'w') as fw:
			fw.write(str(final))	


# In[21]:


def main():
	print("main")
	t_list=['TCS','INFY']
	try:
		while(True):
			for ticker in t_list:
				print("Starting get_quote for ",ticker)
				get_quote(ticker)
			autoSave()
			print("Taking a nap! Good Night")
			time.sleep(30)
			print("\n\n")
	except Exception as e:
		print(e)
	finally:
		with open('infy','a+') as f:
			f.write(str(data_infy))
		with open('tcs','a+') as f:
			f.write(str(data_tcs))
		combiner()
        
main()


# In[ ]:




