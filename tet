import requests
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta

# YARN ResourceManager URL
yarn_url = "http://<yarn-resource-manager-url>:8088/cluster/apps"

# Set time threshold (3 hours)
time_threshold = timedelta(hours=3)

# Function to parse HTML and extract job details
def get_long_running_jobs_from_html(yarn_url, time_threshold):
    try:
        response = requests.get(yarn_url, params={"state": "RUNNING"})
        
        # Check if the response status is OK
        if response.status_code != 200:
            print(f"Failed to retrieve data: {response.status_code}")
            return pd.DataFrame()
        
        # Parse the HTML content
        soup = BeautifulSoup(response.text, "html.parser")
        
        # Locate the job information table or div (modify based on actual HTML structure)
        # Assuming jobs are listed in a table with rows (<tr>) and each job in a row
        job_rows = soup.find_all("tr")  # Adjust selector as needed
        
        # Collect job details
        long_running_jobs = []
        for row in job_rows[1:]:  # Skip header row if present
            columns = row.find_all("td")  # Modify if jobs are listed in a different structure
            
            if len(columns) < 5:  # Adjust based on expected number of columns
                continue
            
            # Extract job details
            job_id = columns[0].text.strip()     # Example: Job ID
            job_name = columns[1].text.strip()   # Example: Job Name
            start_time_str = columns[2].text.strip()  # Example: Start Time
            
            # Parse start time (assuming timestamp format like "yyyy-MM-dd HH:mm:ss")
            try:
                start_time = datetime.strptime(start_time_str, "%Y-%m-%d %H:%M:%S")
            except ValueError:
                continue
            
            # Calculate duration and check if it exceeds the threshold
            duration = datetime.now() - start_time
            if duration > time_threshold:
                # Append job details if running longer than threshold
                long_running_jobs.append({
                    "Job ID": job_id,
                    "Job Name": job_name,
                    "Start Time": start_time,
                    "Duration": duration
                })
        
        # Convert to DataFrame
        df = pd.DataFrame(long_running_jobs)
        return df

    except requests.RequestException as e:
        print("Request failed:", e)
        return pd.DataFrame()

# Get long-running jobs and save to CSV
df_long_running = get_long_running_jobs_from_html(yarn_url, time_threshold)
if not df_long_running.empty:
    df_long_running.to_csv("long_running_yarn_jobs.csv", index=False)
    print("Long-running jobs saved to long_running_yarn_jobs.csv")
else:
    print("No long-running jobs found.")
