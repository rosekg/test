# Common properties
kafka.process.flag=true
send.message.flag=true

# Development environment configuration
dev.bootstrap.servers=dev.kafka.server:9092
dev.group.id=dev-group
dev.input.topic=dev-input-topic
dev.output.topic=dev-output-topic
dev.schema.registry.url=http://dev.schema.registry:8081
dev.jaas.config.path=path/to/dev_jaas.conf

# Production environment configuration
prod.bootstrap.servers=prod.kafka.server:9092
prod.group.id=prod-group
prod.input.topic=prod-input-topic
prod.output.topic=prod-output-topic
prod.schema.registry.url=http://prod.schema.registry:8081
prod.jaas.config.path=path/to/prod_jaas.conf

# Pre-production environment configuration
pre.bootstrap.servers=pre.kafka.server:9092
pre.group.id=pre-group
pre.input.topic=pre-input-topic
pre.output.topic=pre-output




----#â„–##############

json

package com.example.util;

import org.apache.avro.generic.GenericRecord;
import org.apache.avro.io.DatumReader;
import org.apache.avro.io.Decoder;
import org.apache.avro.io.DecoderFactory;
import org.apache.avro.specific.SpecificDatumReader;
import org.apache.log4j.Logger;
import java.nio.charset.StandardCharsets;

public class JsonParserUtil {
    private static final Logger logger = Logger.getLogger(JsonParserUtil.class);

    public static String parseAvroMessage(byte[] avroData) {
        try {
            DatumReader<GenericRecord> datumReader = new SpecificDatumReader<>();
            Decoder decoder = DecoderFactory.get().binaryDecoder(avroData, null);
            GenericRecord genericRecord = datumReader.read(null, decoder);
            return genericRecord.toString();
        } catch (Exception e) {
            logger.error("Failed to parse Avro message", e);
            return null;
        }
    }
}



############

package com.example.consumer;

import com.example.producer.KafkaJsonProducer;
import com.example.util.JsonParserUtil;
import org.apache.kafka.clients.consumer.ConsumerConfig;
import org.apache.kafka.clients.consumer.ConsumerRecord;
import org.apache.kafka.clients.consumer.KafkaConsumer;
import org.apache.kafka.clients.consumer.ConsumerRecords;
import org.apache.kafka.common.serialization.StringDeserializer;
import org.apache.log4j.Logger;
import io.confluent.kafka.serializers.KafkaAvroDeserializer;
import io.confluent.kafka.serializers.AbstractKafkaAvroSerDeConfig;

import java.time.Duration;
import java.util.Collections;
import java.util.Properties;

public class KafkaJsonConsumer {
    private static final Logger logger = Logger.getLogger(KafkaJsonConsumer.class);
    private final KafkaConsumer<String, byte[]> consumer;
    private final String topic;
    private final KafkaJsonProducer producer;

    public KafkaJsonConsumer(String bootstrapServers, String groupId, String topic, String schemaRegistryUrl) {
        this.topic = topic;

        Properties props = new Properties();
        props.put(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers);
        props.put(ConsumerConfig.GROUP_ID_CONFIG, groupId);
        props.put(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName());
        props.put(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, KafkaAvroDeserializer.class.getName());
        props.put(AbstractKafkaAvroSerDeConfig.SCHEMA_REGISTRY_URL_CONFIG, schemaRegistryUrl);

        this.consumer = new KafkaConsumer<>(props);
        this.consumer.subscribe(Collections.singletonList(this.topic));
    }

    public void consumeAndProcess(KafkaJsonProducer producer) {
        this.producer = producer;
        while (true) {
            ConsumerRecords<String, byte[]> records = consumer.poll(Duration.ofMillis(1000));
            for (ConsumerRecord<String, byte[]> record : records) {
                try {
                    String jsonMessage = JsonParserUtil.parseAvroMessage(record.value());
                    logger.info("Consumed message: " + jsonMessage);
                    producer.sendMessage(jsonMessage);
                } catch (Exception e) {
                    logger.error("Failed to process message", e);
                }
            }
        }
    }
}


##########

package com.example.producer;

import org.apache.kafka.clients.producer.KafkaProducer;
import org.apache.kafka.clients.producer.ProducerRecord;
import org.apache.kafka.clients.producer.RecordMetadata;
import org.apache.kafka.common.serialization.StringSerializer;
import org.apache.log4j.Logger;
import io.confluent.kafka.serializers.KafkaAvroSerializer;

import java.util.Properties;
import java.util.concurrent.ExecutionException;

public class KafkaJsonProducer {
    private static final Logger logger = Logger.getLogger(KafkaJsonProducer.class);
    private final KafkaProducer<String, String> producer;
    private final String topic;

    public KafkaJsonProducer(String bootstrapServers, String topic, String schemaRegistryUrl) {
        this.topic = topic;

        Properties props = new Properties();
        props.put("bootstrap.servers", bootstrapServers);
        props.put("key.serializer", StringSerializer.class.getName());
        props.put("value.serializer", KafkaAvroSerializer.class.getName());
        props.put("schema.registry.url", schemaRegistryUrl);

        this.producer = new KafkaProducer<>(props);
    }

    public void sendMessage(String message) {
        try {
            ProducerRecord<String, String> record = new ProducerRecord<>(topic, message);
            RecordMetadata metadata = producer.send(record).get();
            logger.info("Message sent to topic " + metadata.topic() + " partition " + metadata.partition() + " offset " + metadata.offset());
        } catch (InterruptedException | ExecutionException e) {
            logger.error("Failed to send message", e);
        }
    }

    public void close() {
        producer.close();
    }
}


#########
package com.example;

import com.example.consumer.KafkaJsonConsumer;
import com.example.producer.KafkaJsonProducer;
import org.apache.log4j.Logger;
import org.apache.log4j.PropertyConfigurator;

import java.io.FileInputStream;
import java.io.IOException;
import java.util.Properties;

public class MainApp {
    private static final Logger logger = Logger.getLogger(MainApp.class);

    public static void main(String[] args) {
        String environment = (args.length > 0 && !args[0].isEmpty()) ? args[0] : "local";
        String propertyFilePath = (args.length > 1) ? args[1] : "config.properties";

        // Load log4j configuration
        PropertyConfigurator.configure("log4j.properties");

        Properties properties = new Properties();
        try (FileInputStream fis = new FileInputStream(propertyFilePath)) {
            properties.load(fis);
        } catch (IOException e) {
            logger.error("Failed to load properties file: " + propertyFilePath, e);
            System.exit(1);
        }

        // Load environment-specific properties, fallback to local values if environment properties are missing
        Properties envProperties = getEnvironmentProperties(properties, environment);

        // Set the system property for the JAAS configuration file
        String jaasConfigPath = envProperties.getProperty("jaas.config.path");
        if (jaasConfigPath == null || jaasConfigPath.isEmpty()) {
            logger.error("JAAS config path not specified for environment: " + environment);
            System.exit(1);
        }
        System.setProperty("java.security.auth.login.config", jaasConfigPath);

        // Boolean flag to control Kafka processing
        boolean kafkaProcessFlag = Boolean.parseBoolean(envProperties.getProperty("kafka.process.flag", "true"));
        boolean sendMessageFlag = Boolean.parseBoolean(envProperties.getProperty("send.message.flag", "true"));

        if (kafkaProcessFlag) {
            String bootstrapServers = envProperties.getProperty("bootstrap.servers", "localhost:9092");
            String groupId = envProperties.getProperty("group.id", "local-group");
            String inputTopic = envProperties.getProperty("input.topic", "local-input-topic");
            String outputTopic = envProperties.getProperty("output.topic", "local-output-topic");
            String schemaRegistryUrl = envProperties.getProperty("schema.registry.url", "http://localhost:8081");

            KafkaJsonProducer producer = new KafkaJsonProducer(bootstrapServers, outputTopic, schemaRegistryUrl);
            KafkaJsonConsumer consumer = new KafkaJsonConsumer(bootstrapServers, groupId, inputTopic, schemaRegistryUrl);

            // Start consuming and processing messages
            new Thread(() -> {
                consumer.consumeAndProcess(producer);
            }).start();

            // Send a message if the flag is true
            if (sendMessageFlag) {
                String jsonMessage = "{\"key\": \"exampleKey\", \"value\": \"exampleValue\"}";
                producer.sendMessage(jsonMessage);
            }

            Runtime.getRuntime().addShutdownHook(new Thread(() -> {
                producer.close();
            }));
        } else {
            logger.info("Kafka processing flag is set to false. Skipping processing.");
        }
    }

    private static Properties getEnvironmentProperties(Properties properties, String environment) {
        Properties envProperties = new Properties();
        properties.forEach((key, value) -> {
            String keyStr = key.toString();
            if (keyStr.startsWith(environment + ".")) {
                envProperties.put(keyStr.substring(environment.length() + 1), value);
            }
        });
        return envProperties;
    }
}


#######
<project xmlns="http://maven.apache.org/POM/4.0.0"
         xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
         xsi:schemaLocation="http://maven.apache.org/POM/4.0.0 http://maven.apache.org/xsd/maven-4.0.0.xsd">
    <modelVersion>4.0.0</modelVersion>

    <groupId>com.example</groupId>
    <artifactId>kafka-avro-example</artifactId>
    <version>1.0-SNAPSHOT</version>

    <properties>
        <maven.compiler.source>17</maven.compiler.source>
        <maven.compiler.target>17</maven.compiler.target>
    </properties>

    <dependencies>
        <!-- Kafka dependencies -->
        <dependency>
            <groupId>org.apache.kafka</groupId>
            <artifactId>kafka-clients</artifactId>
            <version>3.5.0</version>
        </dependency>
        
        <!-- Confluent Kafka dependencies -->
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-avro-serializer</artifactId>
            <version>7.5.0</version>
        </dependency>
        <dependency>
            <groupId>io.confluent</groupId>
            <artifactId>kafka-schema-registry-client</artifactId>
            <version>7.5.0</version>
        </dependency>
        
        <!-- Avro dependencies -->
        <dependency>
            <groupId>org.apache.avro</groupId>
            <artifactId>avro</artifactId>
            <version>1.11.0</version>
        </dependency>

        <!-- Log4j dependencies -->
        <dependency>
            <groupId>log4j</groupId>
            <artifactId>log4j</artifactId>
            <version>1.2.17</version>
        </dependency>
    </dependencies>

    <build>
        <plugins>
            <!-- Compiler plugin for Java 17 -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-compiler-plugin</artifactId>
                <version>3.8.1</version>
                <configuration>
                    <source>17</source>
                    <target>17</target>
                </configuration>
            </plugin>

            <!-- Shade plugin to create an uber-jar -->
            <plugin>
                <groupId>org.apache.maven.plugins</groupId>
                <artifactId>maven-shade-plugin</artifactId>
                <version>3.2.4</version>
                <executions>
                    <execution>
                        <phase>package</phase>
                        <goals>
                            <goal>shade</goal>
                        </goals>
                        <configuration>
                            <createDependencyReducedPom>false</createDependencyReducedPom>
                            <transformers>
                                <transformer implementation="org.apache.maven.plugins.shade.resource.ManifestResourceTransformer">
                                    <mainClass>com.example.MainApp</mainClass>
                                </transformers>
                            </configuration>
                        </execution>
                    </executions>
                </plugin>
            </plugins>
        </build>
</project>



######
