from elasticsearch import Elasticsearch, helpers

# Define the source and destination Elasticsearch clusters
source_host = 'http://source_host:9200'
destination_host = 'http://destination_host:9200'

# Define the index you want to copy
index_name = 'your_index_name'

# Create Elasticsearch clients for the source and destination clusters with increased timeout
source_es = Elasticsearch(source_host, timeout=300)
destination_es = Elasticsearch(destination_host, timeout=300)

# Function to perform reindexing with retry logic
def reindex_with_slices(source_es, destination_es, index_name, slices, chunk_size, scroll):
    for slice_id in range(slices):
        helpers.reindex(
            client=source_es,
            body={
                "source": {
                    "index": index_name,
                    "slice": {
                        "id": slice_id,
                        "max": slices
                    }
                },
                "dest": {
                    "index": index_name
                }
            },
            target_client=destination_es,
            chunk_size=chunk_size,
            scroll=scroll,
            request_timeout=300  # Request timeout for the bulk indexing
        )
        print(f"Slice {slice_id}/{slices} completed")

# Number of slices (parallel processes)
num_slices = 5  # Adjust based on your cluster's capacity

# Chunk size for bulk indexing
chunk_size = 1000  # Adjust based on your environment

# Scroll duration
scroll = '10m'

# Perform reindexing
reindex_with_slices(source_es, destination_es, index_name, num_slices, chunk_size, scroll)
